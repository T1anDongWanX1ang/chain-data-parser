#!/usr/bin/env python3
"""
ä¼˜åŒ–åçš„åŒºå—é“¾æ•°æ®ç®¡é“æ‰§è¡Œå™¨

ä¸»è¦æ”¹è¿›ï¼š
1. é‡æ„ç»„ä»¶å·¥å‚æ¨¡å¼ï¼Œç»Ÿä¸€ç»„ä»¶åˆ›å»ºé€»è¾‘
2. æ”¯æŒå¤šæ¡ dict_mapper é…ç½®
3. æ”¹è¿›æ•°æ®æµå¤„ç†é€»è¾‘ï¼Œæ”¯æŒæ›´çµæ´»çš„ç»„ä»¶é“¾
4. å¢å¼ºé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
5. ä¼˜åŒ–é…ç½®éªŒè¯å’Œç»„ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†
"""

import asyncio
import json
import os
import sys
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Callable, Union
from loguru import logger
import uuid
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æ•°æ®åº“ç›¸å…³æ¨¡å—
try:
    from app.services.database_service import DatabaseService
    from app.models.contract_abi import ContractAbi
    from sqlalchemy import select
    HAS_DATABASE = True
except ImportError:
    HAS_DATABASE = False

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–å’Œç¼ºå¤±ä¾èµ–é—®é¢˜
# from app.component.event_monitor import ContractEventMonitor, EventMonitorConfig
# from app.component.contract_caller import ContractMethodCaller, MethodCallConfig
# from app.component.dict_mapper import DictMapper, MappingConfig, MappingRule
# from app.component.kafka_client import KafkaClient, KafkaConfig


class ComponentType(Enum):
    """ç»„ä»¶ç±»å‹æšä¸¾"""
    EVENT_MONITOR = "event_monitor"
    CONTRACT_CALLER = "contract_caller"
    DICT_MAPPER = "dict_mapper"
    KAFKA_PRODUCER = "kafka_producer"
    KAFKA_CONSUMER = "kafka_consumer"


@dataclass
class PipelineContext:
    """ç®¡é“ä¸Šä¸‹æ–‡ï¼Œç”¨äºåœ¨ç»„ä»¶é—´ä¼ é€’æ•°æ®"""
    data: Dict[str, Any]
    metadata: Dict[str, Any]
    pipeline_id: str
    task_id: Optional[int] = None  # æ·»åŠ ä»»åŠ¡IDå­—æ®µ
    step_count: int = 0
    
    def add_step_data(self, component_name: str, step_data: Dict[str, Any]):
        """æ·»åŠ æ­¥éª¤æ•°æ®"""
        self.step_count += 1
        self.data[f"step_{self.step_count}_{component_name}"] = step_data
        # å°†æ–°æ•°æ®åˆå¹¶åˆ°ä¸»æ•°æ®ä¸­ï¼ˆä¸è¦†ç›–ç°æœ‰é”®ï¼‰
        for key, value in step_data.items():
            if key not in self.data:
                self.data[key] = value


class PipelineComponent(ABC):
    """ç®¡é“ç»„ä»¶æŠ½è±¡åŸºç±»"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.logger = logger.bind(component=name)
    
    @abstractmethod
    async def execute(self, context: PipelineContext) -> PipelineContext:
        """æ‰§è¡Œç»„ä»¶é€»è¾‘"""
        pass
    
    @abstractmethod
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ç»„ä»¶"""
        pass
    
    @abstractmethod
    async def cleanup(self):
        """æ¸…ç†ç»„ä»¶èµ„æº"""
        pass


class EventMonitorComponent(PipelineComponent):
    """äº‹ä»¶ç›‘æ§ç»„ä»¶"""
    
    def __init__(self, name: str, config: Dict[str, Any], data_processor: Callable):
        super().__init__(name, config)
        self.data_processor = data_processor
        self.monitor = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–äº‹ä»¶ç›‘æ§å™¨"""
        try:
            # å»¶è¿Ÿå¯¼å…¥
            from app.component.event_monitor import ContractEventMonitor, EventMonitorConfig
            
            # åŠ è½½ABI
            abi = await self._load_abi_file(self.config.get('abi_path', ''))
            
            # åˆ›å»ºç›‘æ§é…ç½®
            monitor_config = EventMonitorConfig(
                mode=self.config.get('mode', 'realtime'),
                events_to_monitor=self.config.get('events_to_monitor'),
                output_format=self.config.get('output_format', 'detailed'),
                poll_interval=self.config.get('poll_interval', 1.0),
                custom_handler=self.data_processor
            )
            
            self.monitor = ContractEventMonitor(
                chain_name=self.config['chain_name'],
                contract_address=self.config['contract_address'],
                abi=abi,
                config=monitor_config,
            )
            
            self.logger.info(f"äº‹ä»¶ç›‘æ§å™¨åˆå§‹åŒ–æˆåŠŸ: {self.config['contract_address']}")
            return True
            
        except Exception as e:
            self.logger.error(f"äº‹ä»¶ç›‘æ§å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def execute(self, context: PipelineContext) -> PipelineContext:
        """å¯åŠ¨äº‹ä»¶ç›‘æ§"""
        if self.monitor:
            # ä¼ é€’ä»»åŠ¡IDç»™ç›‘æ§å™¨
            if hasattr(self.monitor, 'task_id'):
                self.monitor.task_id = context.task_id
            self.logger.info(f"å¯åŠ¨äº‹ä»¶ç›‘æ§... (task_id: {context.task_id})")
            await self.monitor.start_monitoring()
        return context
    
    async def cleanup(self):
        """æ¸…ç†ç›‘æ§å™¨èµ„æº"""
        if self.monitor:
            # è¿™é‡Œå¯ä»¥æ·»åŠ åœæ­¢ç›‘æ§çš„é€»è¾‘
            pass
    
    async def _load_abi_file(self, abi_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½ABIæ–‡ä»¶æˆ–ä»æ•°æ®åº“è¯»å–ABI"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®åº“ABI IDæ ¼å¼ï¼šabi_id:123
            if abi_path.startswith("abi_id:") and HAS_DATABASE:
                abi_id = abi_path.replace("abi_id:", "")
                try:
                    abi_id = int(abi_id)
                    return await self._async_load_abi_from_database(abi_id)
                except ValueError:
                    self.logger.error(f"æ— æ•ˆçš„ABI IDæ ¼å¼: {abi_path}")
                    return []
            
            # ä¼ ç»Ÿæ–‡ä»¶è·¯å¾„æ–¹å¼
            if not os.path.isabs(abi_path):
                # ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
                abi_file = project_root / abi_path
            else:
                abi_file = Path(abi_path)

            if not abi_file.exists():
                self.logger.warning(f"ABIæ–‡ä»¶ä¸å­˜åœ¨: {abi_file}")
                return []

            with open(abi_file, 'r', encoding='utf-8') as f:
                abi_data = json.load(f)

            self.logger.info(f"ABIæ–‡ä»¶åŠ è½½æˆåŠŸ: {abi_file}")
            return abi_data
        except Exception as e:
            self.logger.error(f"ABIåŠ è½½å¤±è´¥: {abi_path}, é”™è¯¯: {e}")
            return []
    
    async def _async_load_abi_from_database(self, abi_id: int) -> List[Dict[str, Any]]:
        """å¼‚æ­¥ä»æ•°æ®åº“åŠ è½½ABIå†…å®¹"""
        try:
            db_service = DatabaseService()
            
            async with db_service.get_session() as session:
                # æŸ¥è¯¢ABIè®°å½•
                stmt = select(ContractAbi).where(ContractAbi.id == abi_id)
                result = await session.execute(stmt)
                abi_record = result.scalar_one_or_none()
                
                if not abi_record:
                    self.logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ABI ID: {abi_id}")
                    return []
                
                # è§£æABIå†…å®¹
                abi_content = abi_record.abi_content
                if isinstance(abi_content, str):
                    abi_data = json.loads(abi_content)
                else:
                    abi_data = abi_content
                    
                self.logger.info(f"ä»æ•°æ®åº“åŠ è½½ABIæˆåŠŸ: {abi_record.contract_name} (ID: {abi_id})")
                return abi_data if isinstance(abi_data, list) else []
                
        except Exception as e:
            self.logger.error(f"ä»æ•°æ®åº“åŠ è½½ABIå¤±è´¥ (ID: {abi_id}): {e}")
            return []


class ContractCallerComponent(PipelineComponent):
    """åˆçº¦è°ƒç”¨ç»„ä»¶ - æ”¯æŒå¤šä¸ªåˆçº¦è°ƒç”¨å™¨"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.callers = {}  # å­˜å‚¨å¤šä¸ªè°ƒç”¨å™¨ï¼Œkeyä¸ºevent_name
        self.single_caller = None  # å•ä¸ªè°ƒç”¨å™¨ï¼ˆå‘åå…¼å®¹ï¼‰
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆçº¦è°ƒç”¨å™¨"""
        try:
            # å»¶è¿Ÿå¯¼å…¥
            from app.component.contract_caller import ContractMethodCaller, MethodCallConfig
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªåˆçº¦è°ƒç”¨å™¨é…ç½®
            contract_callers_config = self.config.get('contract_callers', [])
            
            if contract_callers_config:
                # æ–°æ ¼å¼ï¼šå¤šä¸ªåˆçº¦è°ƒç”¨å™¨
                for caller_config in contract_callers_config:
                    event_name = caller_config.get('event_name')
                    if not event_name:
                        self.logger.warning("åˆçº¦è°ƒç”¨å™¨é…ç½®ç¼ºå°‘ event_nameï¼Œè·³è¿‡")
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ¨æ€åœ°å€ï¼ˆä»¥{å¼€å¤´çš„æ¨¡æ¿ï¼‰
                    contract_address = caller_config['contract_address']
                    if contract_address.startswith('{') and contract_address.endswith('}'):
                        # åŠ¨æ€åœ°å€ï¼Œå»¶è¿Ÿåˆå§‹åŒ–ï¼Œä»…å­˜å‚¨é…ç½®
                        self.callers[event_name] = {
                            'config': caller_config,  # å­˜å‚¨å®Œæ•´é…ç½®ç”¨äºåç»­åˆå§‹åŒ–
                            'caller': None,  # å»¶è¿Ÿåˆ›å»º
                            'method_name': caller_config.get('method_name'),
                            'method_params': caller_config.get('method_params', [])
                        }
                        self.logger.info(f"åˆçº¦è°ƒç”¨å™¨é…ç½®å·²æ³¨å†Œï¼ˆåŠ¨æ€åœ°å€ï¼‰: {event_name} -> {contract_address}")
                        continue
                    
                    # é™æ€åœ°å€ï¼Œç«‹å³åˆå§‹åŒ–
                    # åŠ è½½ABI
                    abi = await self._load_abi_file(caller_config.get('abi_path', ''))
                    
                    # åˆ›å»ºè°ƒç”¨é…ç½®
                    call_config = MethodCallConfig(
                        output_format="json",
                        include_block_info=False
                    )
                    
                    caller = ContractMethodCaller(
                        chain_name=caller_config['chain_name'],
                        contract_address=contract_address,
                        abi=abi,
                        config=call_config
                    )
                    
                    # å­˜å‚¨è°ƒç”¨å™¨é…ç½®
                    self.callers[event_name] = {
                        'caller': caller,
                        'method_name': caller_config.get('method_name'),
                        'method_params': caller_config.get('method_params', [])
                    }
                    
                    self.logger.info(f"åˆçº¦è°ƒç”¨å™¨åˆå§‹åŒ–æˆåŠŸ: {event_name} -> {contract_address}")
                
                return len(self.callers) > 0
            else:
                # æ—§æ ¼å¼ï¼šå•ä¸ªåˆçº¦è°ƒç”¨å™¨ï¼ˆå‘åå…¼å®¹ï¼‰
                abi = await self._load_abi_file(self.config.get('abi_path', ''))
                
                call_config = MethodCallConfig(
                    output_format="json",
                    include_block_info=False
                )
                
                self.single_caller = ContractMethodCaller(
                    chain_name=self.config['chain_name'],
                    contract_address=self.config['contract_address'],
                    abi=abi,
                    config=call_config
                )
                
                self.logger.info(f"å•ä¸ªåˆçº¦è°ƒç”¨å™¨åˆå§‹åŒ–æˆåŠŸ: {self.config['contract_address']}")
                return True
            
        except Exception as e:
            self.logger.error(f"åˆçº¦è°ƒç”¨å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def execute(self, context: PipelineContext) -> PipelineContext:
        """æ‰§è¡Œåˆçº¦è°ƒç”¨ - æ”¯æŒå¤šä¸ªè°ƒç”¨å™¨å¹¶å‘æ‰§è¡Œï¼Œä»¥æ–¹æ³•åä½œä¸ºkey"""
        try:
            if self.config.get('contract_callers'):
                # æ–°æ ¼å¼ï¼šæ•°æ®åº“é…ç½®çš„å¤šä¸ªåˆçº¦è°ƒç”¨å™¨
                await self._execute_database_contract_callers(context)
            elif self.callers:
                # æ—§æ ¼å¼ï¼šåŸºäºevent_nameçš„å¤šä¸ªè°ƒç”¨å™¨
                await self._execute_event_based_callers(context)
            elif self.single_caller:
                # å•ä¸ªåˆçº¦è°ƒç”¨å™¨ï¼ˆå‘åå…¼å®¹ï¼‰
                await self._execute_single_caller(context)
            
            return context
            
        except Exception as e:
            self.logger.error(f"åˆçº¦è°ƒç”¨å¤±è´¥: {e}")
            return context

    async def _execute_database_contract_callers(self, context: PipelineContext):
        """æ‰§è¡Œæ•°æ®åº“é…ç½®çš„å¤šä¸ªåˆçº¦è°ƒç”¨å™¨"""
        contract_callers_config = self.config.get('contract_callers', [])
        component_id = self.config.get('id', hash(self.name) % (10**8))
        
        # è·å–å½“å‰äº‹ä»¶åç§°
        current_event_name = context.data.get('event_name') or context.data.get('event')
        
        # ä¸ºæ¯ä¸ªåˆçº¦è°ƒç”¨å™¨åˆ›å»ºç»“æœå®¹å™¨ï¼Œä»¥æ–¹æ³•åä¸ºkey
        contract_results = {}
        
        self.logger.info(f"å¼€å§‹å¤„ç†æ•°æ®åº“é…ç½®çš„åˆçº¦è°ƒç”¨å™¨ï¼Œå…± {len(contract_callers_config)} ä¸ªï¼Œå½“å‰äº‹ä»¶åç§°: {current_event_name}")
        
        for caller_config in contract_callers_config:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿‡æ»¤ï¼šåªæœ‰å½“ç»„ä»¶é…ç½®äº† event_name æ—¶æ‰è¿›è¡Œè¿‡æ»¤
            config_event_name = caller_config.get('event_name')
            if config_event_name and current_event_name and config_event_name != current_event_name:
                self.logger.info(f"è·³è¿‡ä¸åŒ¹é…çš„åˆçº¦è°ƒç”¨å™¨: é…ç½®äº‹ä»¶={config_event_name}, å½“å‰äº‹ä»¶={current_event_name}")
                continue
            try:
                # è§£ææ–¹æ³•å‚æ•°
                method_params = caller_config.get('method_params', [])
                method_args = []
                if method_params:
                    method_args = [self._resolve_parameter(item, context.data) for item in method_params]
                
                # åˆ›å»ºåˆçº¦è°ƒç”¨å™¨
                abi = await self._load_abi_file(caller_config.get('abi_path', ''))
                if not abi:
                    self.logger.warning(f"ABIæ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œè·³è¿‡åˆçº¦è°ƒç”¨: {caller_config.get('abi_path')}")
                    continue
                
                # å»¶è¿Ÿå¯¼å…¥
                from app.component.contract_caller import ContractMethodCaller, MethodCallConfig
                
                # åˆ›å»ºè°ƒç”¨é…ç½®
                call_config = MethodCallConfig(
                    output_format="json",
                    include_block_info=False
                )
                
                # è§£æåˆçº¦åœ°å€
                contract_address = self._resolve_contract_address(caller_config, context.data)
                
                if not contract_address:
                    self.logger.warning(f"åˆçº¦åœ°å€è§£æå¤±è´¥ï¼Œè·³è¿‡è°ƒç”¨: {caller_config}")
                    continue
                
                # åˆ›å»ºåˆçº¦æ–¹æ³•è°ƒç”¨å™¨
                contract_caller = ContractMethodCaller(
                    chain_name=caller_config.get('chain_name'),
                    contract_address=contract_address,
                    abi=abi,
                    config=call_config
                )
                
                # æ‰§è¡Œåˆçº¦è°ƒç”¨
                method_name = caller_config.get('method_name')
                call_result = contract_caller.call_method(method_name, method_args)
                
                # ç»Ÿä¸€ä½¿ç”¨æ–¹æ³•åä½œä¸ºkey
                if method_name:
                    # ç›´æ¥ä½¿ç”¨æ–¹æ³•åä½œä¸ºkeyï¼Œä¸ç®¡æ˜¯å¦æœ‰å‚æ•°
                    formatted_result = call_result.get('result') if isinstance(call_result, dict) else call_result
                    
                    if method_name in contract_results:
                        # å¦‚æœæ–¹æ³•åé‡å¤ï¼Œåˆ›å»ºæ•°ç»„
                        if not isinstance(contract_results[method_name], list):
                            contract_results[method_name] = [contract_results[method_name]]
                        contract_results[method_name].append(formatted_result)
                    else:
                        contract_results[method_name] = formatted_result
                
                # ä¸éœ€è¦ä¿å­˜åˆ°æ•°æ®åº“ï¼Œç›´æ¥å¤„ç†ç»“æœ
                
                self.logger.info(f"åˆçº¦è°ƒç”¨æˆåŠŸ: {method_name}, å‚æ•°: {method_args}, ç»“æœ: {call_result}")
                
            except Exception as e:
                method_name = caller_config.get('method_name', 'unknown')
                
                # é”™è¯¯æƒ…å†µä¸‹ç»Ÿä¸€ä½¿ç”¨nullå€¼
                error_result = None
                
                contract_results[method_name] = error_result
                self.logger.error(f"åˆçº¦è°ƒç”¨å¤±è´¥: {method_name}, é”™è¯¯: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–è°ƒç”¨å™¨ï¼Œä¸å› å•ä¸ªå¤±è´¥è€Œä¸­æ–­
                continue
        
        # å°†æ‰€æœ‰åˆçº¦è°ƒç”¨ç»“æœåˆå¹¶åˆ°ä¸Šä¸‹æ–‡æ•°æ®ä¸­
        if contract_results:
            # å°†ç»“æœæŒ‰æ–¹æ³•åç»„è£…åˆ°æ•°æ®ä¸­
            context.data.update(contract_results)
            
            # åŒæ—¶ä¿ç•™åŸæœ‰çš„ç»„ä»¶åæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            context.add_step_data(self.name, contract_results)
            
            self.logger.info(f"åˆçº¦è°ƒç”¨å™¨å¤„ç†å®Œæˆï¼Œç»“æœkeys: {list(contract_results.keys())}")

    async def _execute_event_based_callers(self, context: PipelineContext):
        """æ‰§è¡ŒåŸºäºevent_nameçš„è°ƒç”¨å™¨ï¼ˆæ—§æ ¼å¼å…¼å®¹ï¼‰"""
        event_name = context.data.get('event_name')
        if not event_name:
            self.logger.warning("æ•°æ®ä¸­ç¼ºå°‘ event_name å­—æ®µï¼Œæ— æ³•é€‰æ‹©åˆçº¦è°ƒç”¨å™¨")
            return
        
        caller_info = self.callers.get(event_name)
        if not caller_info:
            self.logger.warning(f"æœªæ‰¾åˆ° event_name '{event_name}' å¯¹åº”çš„åˆçº¦è°ƒç”¨å™¨")
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆåŠ¨æ€åœ°å€ï¼‰
        caller = caller_info['caller']
        if caller is None and 'config' in caller_info:
            # å»¶è¿Ÿåˆå§‹åŒ–ï¼šæ ¹æ®å½“å‰äº‹ä»¶æ•°æ®è§£æåŠ¨æ€åœ°å€
            caller_config = caller_info['config']
            contract_address = caller_config['contract_address']
            
            # è§£æåŠ¨æ€åœ°å€
            resolved_address = self._resolve_parameter(contract_address, context.data)
            if not resolved_address:
                self.logger.error(f"æ— æ³•è§£æåŠ¨æ€åœ°å€: {contract_address}")
                return
            
            # åŠ è½½ABIå¹¶åˆ›å»ºè°ƒç”¨å™¨
            try:
                abi = await self._load_abi_file(caller_config.get('abi_path', ''))
                if not abi:
                    self.logger.error(f"ABIæ–‡ä»¶åŠ è½½å¤±è´¥: {caller_config.get('abi_path')}")
                    return
                
                # å»¶è¿Ÿå¯¼å…¥
                from app.component.contract_caller import ContractMethodCaller, MethodCallConfig
                
                call_config = MethodCallConfig(
                    output_format="json",
                    include_block_info=False
                )
                
                caller = ContractMethodCaller(
                    chain_name=caller_config['chain_name'],
                    contract_address=resolved_address,
                    abi=abi,
                    config=call_config
                )
                
                # æ›´æ–°è°ƒç”¨å™¨ä¿¡æ¯ï¼Œä½†ä¸æŒä¹…åŒ–ï¼ˆæ¯æ¬¡åŠ¨æ€åˆ›å»ºï¼‰
                caller_info['caller'] = caller
                self.logger.info(f"åŠ¨æ€åœ°å€è°ƒç”¨å™¨åˆ›å»ºæˆåŠŸ: {event_name} -> {resolved_address}")
                
            except Exception as e:
                self.logger.error(f"åŠ¨æ€è°ƒç”¨å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                return
        
        # æ‰§è¡Œåˆçº¦è°ƒç”¨
        method_name = caller_info['method_name']
        method_params = caller_info['method_params']
        
        # è§£æå‚æ•°
        method_args = []
        if method_params:
            method_args = [self._resolve_parameter(param, context.data) for param in method_params]
        
        # è°ƒç”¨åˆçº¦æ–¹æ³•
        call_result = caller.call_method(method_name, method_args)
        
        # å°†ç»“æœä»¥æ–¹æ³•åä½œä¸ºkeyæ·»åŠ åˆ°ä¸Šä¸‹æ–‡æ•°æ®ä¸­
        if method_name and call_result:
            context.data[method_name] = call_result
        
        # åŒæ—¶ä¿ç•™å®Œæ•´çš„è°ƒç”¨ç»“æœ
        context.add_step_data(self.name, call_result)
        
        self.logger.info(f"åˆçº¦è°ƒç”¨æˆåŠŸ: {event_name} -> {method_name}, ç»“æœå·²æ·»åŠ åˆ°æ•°æ®ä¸­")

    async def _execute_single_caller(self, context: PipelineContext):
        """æ‰§è¡Œå•ä¸ªåˆçº¦è°ƒç”¨å™¨ï¼ˆå‘åå…¼å®¹ï¼‰"""
        method_name = self.config.get('method_name')
        method_params = self.config.get('method_params', [])
        
        # è§£æå‚æ•°
        method_args = []
        if method_params:
            method_args = [self._resolve_parameter(param, context.data) for param in method_params]
        
        # è°ƒç”¨åˆçº¦æ–¹æ³•
        call_result = self.single_caller.call_method(method_name, method_args)
        
        # ä»¥æ–¹æ³•åä½œä¸ºkeyæ·»åŠ ç»“æœåˆ°ä¸Šä¸‹æ–‡
        if method_name and call_result:
            context.data[method_name] = call_result
        
        # æ·»åŠ å®Œæ•´ç»“æœåˆ°ä¸Šä¸‹æ–‡
        context.add_step_data(self.name, call_result)
        
        self.logger.info(f"å•ä¸ªåˆçº¦è°ƒç”¨æˆåŠŸ: {method_name}")

    
    async def cleanup(self):
        """æ¸…ç†è°ƒç”¨å™¨èµ„æº"""
        pass
    
    async def _load_abi_file(self, abi_path: str) -> List[Dict[str, Any]]:
        """åŠ è½½ABIæ–‡ä»¶æˆ–ä»æ•°æ®åº“è¯»å–ABI"""
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®åº“ABI IDæ ¼å¼ï¼šabi_id:123
            if abi_path.startswith("abi_id:") and HAS_DATABASE:
                abi_id = abi_path.replace("abi_id:", "")
                try:
                    abi_id = int(abi_id)
                    return await self._async_load_abi_from_database(abi_id)
                except ValueError:
                    self.logger.error(f"æ— æ•ˆçš„ABI IDæ ¼å¼: {abi_path}")
                    return []
            
            # ä¼ ç»Ÿæ–‡ä»¶è·¯å¾„æ–¹å¼
            if not os.path.isabs(abi_path):
                abi_file = project_root / abi_path
            else:
                abi_file = Path(abi_path)

            if not abi_file.exists():
                self.logger.warning(f"ABIæ–‡ä»¶ä¸å­˜åœ¨: {abi_file}")
                return []

            with open(abi_file, 'r', encoding='utf-8') as f:
                abi_data = json.load(f)

            return abi_data
        except Exception as e:
            self.logger.error(f"ABIåŠ è½½å¤±è´¥: {abi_path}, é”™è¯¯: {e}")
            return []
    
    async def _async_load_abi_from_database(self, abi_id: int) -> List[Dict[str, Any]]:
        """å¼‚æ­¥ä»æ•°æ®åº“åŠ è½½ABIå†…å®¹"""
        try:
            db_service = DatabaseService()
            
            async with db_service.get_session() as session:
                # æŸ¥è¯¢ABIè®°å½•
                stmt = select(ContractAbi).where(ContractAbi.id == abi_id)
                result = await session.execute(stmt)
                abi_record = result.scalar_one_or_none()
                
                if not abi_record:
                    self.logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ABI ID: {abi_id}")
                    return []
                
                # è§£æABIå†…å®¹
                abi_content = abi_record.abi_content
                if isinstance(abi_content, str):
                    abi_data = json.loads(abi_content)
                else:
                    abi_data = abi_content
                    
                self.logger.info(f"ä»æ•°æ®åº“åŠ è½½ABIæˆåŠŸ: {abi_record.contract_name} (ID: {abi_id})")
                return abi_data if isinstance(abi_data, list) else []
                
        except Exception as e:
            self.logger.error(f"ä»æ•°æ®åº“åŠ è½½ABIå¤±è´¥ (ID: {abi_id}): {e}")
            return []
    
    def _resolve_parameter(self, param: str, data: Dict[str, Any]) -> Any:
        """è§£æå‚æ•°å¼•ç”¨"""
        if not isinstance(param, str):
            return param

        # è§£æå½¢å¦‚ "args.to" æˆ– "event_args.to" çš„å¼•ç”¨
        if '.' in param:
            keys = param.split('.')
            current = data

            for key in keys:
                if isinstance(current, dict) and key in current:
                    current = current[key]
                else:
                    self.logger.warning(f"æ— æ³•è§£æå‚æ•°å¼•ç”¨: {param}")
                    return None

            return current

        # ç›´æ¥é”®å¼•ç”¨
        return data.get(param)
    
    def _resolve_contract_address(self, caller_config: Dict[str, Any], data: Dict[str, Any]) -> Optional[str]:
        """è§£æåˆçº¦åœ°å€ï¼Œæ”¯æŒé™æ€åœ°å€å’ŒåŠ¨æ€å­—æ®µå¼•ç”¨"""
        address_source = caller_config.get('contract_address_source', 'static')
        
        if address_source == 'static':
            # é™æ€åœ°å€ï¼šç›´æ¥è¿”å›é…ç½®çš„åœ°å€
            return caller_config.get('contract_address')
        
        elif address_source == 'dynamic':
            # åŠ¨æ€åœ°å€ï¼šä»JSONæ•°æ®ä¸­è§£æ
            address_field = caller_config.get('contract_address_field')
            
            if not address_field:
                self.logger.error("åŠ¨æ€åˆçº¦åœ°å€é…ç½®ç¼ºå°‘ contract_address_field")
                return None
            
            # ä½¿ç”¨ç°æœ‰çš„å‚æ•°è§£æé€»è¾‘
            resolved_address = self._resolve_parameter(address_field, data)
            
            if not resolved_address:
                self.logger.error(f"æ— æ³•ä»æ•°æ®ä¸­è§£æåˆçº¦åœ°å€: {address_field}")
                return None
            
            # ç¡®ä¿åœ°å€æ ¼å¼æ­£ç¡®
            if isinstance(resolved_address, str) and resolved_address.startswith('0x'):
                return resolved_address
            else:
                self.logger.error(f"è§£æçš„åˆçº¦åœ°å€æ ¼å¼ä¸æ­£ç¡®: {resolved_address}")
                return None
        
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„åˆçº¦åœ°å€æ¥æºç±»å‹: {address_source}")
            return None


class DictMapperComponent(PipelineComponent):
    """å­—å…¸æ˜ å°„ç»„ä»¶ - æ”¯æŒå¤šæ¡æ˜ å°„å™¨"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.mappers = []  # å­˜å‚¨å¤šä¸ªæ˜ å°„å™¨
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–å­—å…¸æ˜ å°„å™¨"""
        try:
            # å»¶è¿Ÿå¯¼å…¥
            from app.component.dict_mapper import DictMapper, MappingConfig, MappingRule
            
            # æ”¯æŒæ–°çš„å¤šæ¡é…ç½®æ ¼å¼
            dict_mappers_config = self.config.get('dict_mappers', [])
            
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ dict_mappers ä½†æœ‰ mapping_rules
            if not dict_mappers_config and self.config.get('mapping_rules'):
                dict_mappers_config = [{
                    'event_name': None,
                    'mapping_rules': self.config.get('mapping_rules', [])
                }]
            
            # åˆ›å»ºå¤šä¸ªæ˜ å°„å™¨
            for i, mapper_config in enumerate(dict_mappers_config):
                event_name = mapper_config.get('event_name')
                mapping_rules = mapper_config.get('mapping_rules', [])
                
                # åˆ›å»ºæ˜ å°„è§„åˆ™
                rules = []
                for rule_config in mapping_rules:
                    rule = MappingRule(
                        source_key=rule_config['source_key'],
                        target_key=rule_config['target_key'],
                        transformer=rule_config.get('transformer'),
                        condition=rule_config.get('condition'),
                        default_value=rule_config.get('default_value')
                    )
                    rules.append(rule)
                
                # åˆ›å»ºæ˜ å°„é…ç½®ï¼Œä½¿ç”¨åŒºå—é“¾é…ç½®è·å–æ‰€æœ‰è½¬æ¢å™¨ï¼ˆåŒ…æ‹¬decimal_normalize_with_fieldï¼‰
                from .dict_mapper import CommonMappingConfigs
                blockchain_config = CommonMappingConfigs.blockchain_config()
                config = MappingConfig(
                    mapping_rules=rules,
                    transformers=blockchain_config.transformers,
                    validators=blockchain_config.validators
                )
                
                # åˆ›å»ºæ˜ å°„å™¨
                mapper = DictMapper(config)
                
                self.mappers.append({
                    'event_name': event_name,
                    'mapper': mapper,
                    'rules_count': len(rules)
                })
            
            self.logger.info(f"å­—å…¸æ˜ å°„å™¨åˆå§‹åŒ–æˆåŠŸ: åˆ›å»ºäº† {len(self.mappers)} ä¸ªæ˜ å°„å™¨")
            return True
            
        except Exception as e:
            self.logger.error(f"å­—å…¸æ˜ å°„å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def execute(self, context: PipelineContext) -> PipelineContext:
        """æ‰§è¡Œå­—å…¸æ˜ å°„"""
        try:
            # è·å–å½“å‰äº‹ä»¶åç§°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            current_event_name = context.data.get('event_name') or context.data.get('event')
            
            # åˆå§‹åŒ–ç»“æœæ•°æ®ï¼Œåªä¿ç•™æ˜ å°„åçš„å­—æ®µ
            final_mapped_data = {}
            
            # æ‰§è¡Œæ‰€æœ‰é€‚ç”¨çš„æ˜ å°„å™¨
            for mapper_info in self.mappers:
                event_name = mapper_info['event_name']
                mapper = mapper_info['mapper']
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥åº”ç”¨æ­¤æ˜ å°„å™¨
                should_apply = (
                    event_name is None or  # é€šç”¨æ˜ å°„å™¨
                    event_name == current_event_name or  # åŒ¹é…ç‰¹å®šäº‹ä»¶
                    current_event_name is None  # æ²¡æœ‰äº‹ä»¶åç§°æ—¶åº”ç”¨æ‰€æœ‰æ˜ å°„å™¨
                )
                
                if should_apply:
                    try:
                        # ä½¿ç”¨åŸå§‹æ•°æ®è¿›è¡Œæ˜ å°„
                        mapped_result = mapper.map_dict(context.data)
                        
                        # åªä¿ç•™æ˜ å°„åçš„å­—æ®µï¼Œä¸åˆå¹¶åŸå§‹æ•°æ®
                        final_mapped_data.update(mapped_result)
                        
                        self.logger.info(f"åº”ç”¨æ˜ å°„å™¨æˆåŠŸ: event_name={event_name}, è§„åˆ™æ•°={mapper_info['rules_count']}, æ˜ å°„å­—æ®µæ•°={len(mapped_result)}")
                    except Exception as e:
                        self.logger.error(f"æ˜ å°„å™¨æ‰§è¡Œå¤±è´¥: event_name={event_name}, é”™è¯¯: {e}")
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•æ˜ å°„ç»“æœï¼Œè®°å½•è­¦å‘Š
            if not final_mapped_data:
                self.logger.warning(f"å­—å…¸æ˜ å°„æœªäº§ç”Ÿä»»ä½•ç»“æœ: event_name={current_event_name}")
            
            # æ›´æ–°ä¸Šä¸‹æ–‡æ•°æ®ï¼Œåªä¿ç•™æ˜ å°„åçš„å­—æ®µ
            context.add_step_data(self.name, final_mapped_data)
            context.data = final_mapped_data
            
            return context
            
        except Exception as e:
            self.logger.error(f"å­—å…¸æ˜ å°„æ‰§è¡Œå¤±è´¥: {e}")
            return context
    
    async def cleanup(self):
        """æ¸…ç†æ˜ å°„å™¨èµ„æº"""
        self.mappers.clear()


class KafkaProducerComponent(PipelineComponent):
    """Kafkaç”Ÿäº§è€…ç»„ä»¶"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.kafka_client = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Kafkaå®¢æˆ·ç«¯"""
        try:
            # å»¶è¿Ÿå¯¼å…¥
            from app.component.kafka_client import KafkaClient, KafkaConfig
            
            # åˆ›å»ºKafkaé…ç½®
            kafka_config = KafkaConfig(
                bootstrap_servers=self.config.get('bootstrap_servers', 'localhost:9092'),
                acks=self.config.get('acks', 1),
                sync_send=self.config.get('sync_send', False),
                retries=self.config.get('retries', 3),
                batch_size=self.config.get('batch_size', 16384),
                linger_ms=self.config.get('linger_ms', 0),
                message_format=self.config.get('message_format', 'simple')  # é»˜è®¤åªå‘é€æ•°æ®éƒ¨åˆ†
            )
            
            self.kafka_client = KafkaClient(kafka_config)
            self.kafka_client.init_producer()
            
            self.logger.info(f"Kafkaç”Ÿäº§è€…åˆå§‹åŒ–æˆåŠŸ: {self.config.get('bootstrap_servers')}")
            return True
            
        except Exception as e:
            self.logger.error(f"Kafkaç”Ÿäº§è€…åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def execute(self, context: PipelineContext) -> PipelineContext:
        """å‘é€æ¶ˆæ¯åˆ°Kafka"""
        try:
            topic = self.config.get('topic')
            if not topic:
                self.logger.error("æœªé…ç½®Kafkaä¸»é¢˜")
                return context
            
            # æ‰“å°è¦å‘é€åˆ°Kafkaçš„æ•°æ®
            import json
            self.logger.info(f"å‡†å¤‡å‘é€åˆ°Kafka - topic: {topic}")
            self.logger.info(f"å‘é€æ•°æ®: {json.dumps(context.data, ensure_ascii=False, indent=2)}")
            
            # å‘é€æ¶ˆæ¯
            success = self.kafka_client.send_message(topic, context.data)
            
            if success:
                self.logger.info(f"âœ… Kafkaæ¶ˆæ¯å‘é€æˆåŠŸ: topic={topic}")
            else:
                self.logger.error(f"âŒ Kafkaæ¶ˆæ¯å‘é€å¤±è´¥: topic={topic}")
            
            return context
            
        except Exception as e:
            self.logger.error(f"Kafkaæ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            return context
    
    async def cleanup(self):
        """æ¸…ç†Kafkaå®¢æˆ·ç«¯èµ„æº"""
        if self.kafka_client:
            # è¿™é‡Œå¯ä»¥æ·»åŠ å…³é—­ç”Ÿäº§è€…çš„é€»è¾‘
            pass


class ComponentFactory:
    """ç»„ä»¶å·¥å‚ç±»"""
    
    @staticmethod
    def create_component(comp_type: str, name: str, config: Dict[str, Any], **kwargs) -> PipelineComponent:
        """åˆ›å»ºç»„ä»¶å®ä¾‹"""
        try:
            component_type = ComponentType(comp_type)
            
            if component_type == ComponentType.EVENT_MONITOR:
                data_processor = kwargs.get('data_processor')
                if not data_processor:
                    raise ValueError("äº‹ä»¶ç›‘æ§å™¨éœ€è¦ data_processor å‚æ•°")
                return EventMonitorComponent(name, config, data_processor)
            
            elif component_type == ComponentType.CONTRACT_CALLER:
                return ContractCallerComponent(name, config)
            
            elif component_type == ComponentType.DICT_MAPPER:
                return DictMapperComponent(name, config)
            
            elif component_type == ComponentType.KAFKA_PRODUCER:
                return KafkaProducerComponent(name, config)
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç»„ä»¶ç±»å‹: {comp_type}")
                
        except ValueError as e:
            logger.error(f"åˆ›å»ºç»„ä»¶å¤±è´¥: {e}")
            raise


class OptimizedBlockchainDataPipeline:
    """ä¼˜åŒ–åçš„åŒºå—é“¾æ•°æ®ç®¡é“æ‰§è¡Œå™¨"""

    def __init__(self, config_path: str = None, config_dict: Dict[str, Any] = None, log_path: str = None, task_id: int = None):
        """
        åˆå§‹åŒ–ç®¡é“
        
        Args:
            config_path: JSONé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            config_dict: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
        """
        self.config_path = config_path
        self.config = None
        self.components: List[PipelineComponent] = []
        self.log_path = log_path
        self.task_id = task_id
        
        # ä¸ºæ¯ä¸ªå®ä¾‹åˆ›å»ºå”¯ä¸€çš„æ ‡è¯†
        self.instance_id = str(uuid.uuid4())[:8]
        self.logger_ids = []
        
        # é…ç½®æ—¥å¿—è¾“å‡º
        self._setup_logging()
        
        # åŠ è½½é…ç½®
        if config_dict:
            self.config = config_dict
            logger.bind(instance_id=self.instance_id).info(f"ä»é…ç½®å­—å…¸åŠ è½½ç®¡é“é…ç½® - {self.config.get('pipeline_name', 'unknown')}")
        elif config_path:
            self._load_config()
        else:
            raise ValueError("å¿…é¡»æä¾› config_path æˆ– config_dict å‚æ•°")

        # éªŒè¯é…ç½®
        self._validate_config()
        
        logger.bind(instance_id=self.instance_id).info(f"ä¼˜åŒ–ç‰ˆåŒºå—é“¾æ•°æ®ç®¡é“åˆå§‹åŒ–å®Œæˆ - {self.config.get('pipeline_name', 'unknown')}")
        if self.log_path:
            logger.bind(instance_id=self.instance_id).info(f"æ—¥å¿—è¾“å‡ºè·¯å¾„: {self.log_path}")

    def _load_config(self):
        """åŠ è½½JSONé…ç½®æ–‡ä»¶"""
        try:
            config_file = Path(self.config_path)
            if not config_file.exists():
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")

            with open(config_file, 'r', encoding='utf-8') as f:
                self.config = json.load(f)

            logger.bind(instance_id=self.instance_id).info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise

    def _validate_config(self):
        """éªŒè¯é…ç½®æ–‡ä»¶"""
        if not self.config:
            raise ValueError("é…ç½®ä¸ºç©º")
        
        components = self.config.get('components', [])
        if not components:
            raise ValueError("ç»„ä»¶é…ç½®ä¸ºç©º")
        
        # éªŒè¯ç¬¬ä¸€ä¸ªç»„ä»¶å¿…é¡»æ˜¯æ•°æ®æºç»„ä»¶
        first_component = components[0]
        first_type = first_component.get('type')
        
        if first_type not in ['event_monitor', 'kafka_consumer']:
            raise ValueError(f"ç¬¬ä¸€ä¸ªç»„ä»¶å¿…é¡»æ˜¯æ•°æ®æºç»„ä»¶(event_monitor/kafka_consumer)ï¼Œå½“å‰ä¸º: {first_type}")
        
        logger.bind(instance_id=self.instance_id).info(f"é…ç½®éªŒè¯é€šè¿‡: {len(components)} ä¸ªç»„ä»¶")

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—è¾“å‡º - åŒæ—¶æ”¯æŒæ§åˆ¶å°å’Œæ–‡ä»¶è¾“å‡º"""
        # å­˜å‚¨æ—¥å¿—å¤„ç†å™¨IDåˆ—è¡¨
        self.logger_ids = []
        
        # å®šä¹‰æ—¥å¿—æ ¼å¼
        # å¸¦å®ä¾‹IDçš„æ—¥å¿—æ ¼å¼
        instance_log_format = "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>[{extra[instance_id]}]</cyan> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
        # æ™®é€šæ—¥å¿—æ ¼å¼
        general_log_format = "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>[global]</cyan> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
        
        # æ·»åŠ æ§åˆ¶å°è¾“å‡º - å¸¦å®ä¾‹IDçš„æ—¥å¿—
        instance_console_handler_id = logger.add(
            sys.stdout,
            format=instance_log_format,
            level="INFO",
            filter=lambda record: record["extra"].get("instance_id") == self.instance_id
        )
        self.logger_ids.append(instance_console_handler_id)
        
        # æ·»åŠ æ§åˆ¶å°è¾“å‡º - æ™®é€šæ—¥å¿—ï¼ˆæ²¡æœ‰å®ä¾‹IDçš„ï¼‰
        general_console_handler_id = logger.add(
            sys.stdout,
            format=general_log_format,
            level="INFO",
            filter=lambda record: record["extra"].get("instance_id") is None
        )
        self.logger_ids.append(general_console_handler_id)
        
        # å¦‚æœæŒ‡å®šäº†æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ŒåŒæ—¶æ·»åŠ æ–‡ä»¶è¾“å‡º
        if self.log_path:
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™ç›¸å¯¹äºå·¥ç¨‹æ ¹ç›®å½•
            if not os.path.isabs(self.log_path):
                # è·å–å·¥ç¨‹æ ¹ç›®å½•ï¼ˆå½“å‰æ–‡ä»¶çš„ä¸Šä¸Šçº§ç›®å½•ï¼‰
                project_root = Path(__file__).parent.parent.parent
                log_file = project_root / self.log_path
            else:
                log_file = Path(self.log_path)
            
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            # æ·»åŠ æ–‡ä»¶è¾“å‡º - å¸¦å®ä¾‹IDçš„æ—¥å¿—
            instance_file_handler_id = logger.add(
                str(log_file),
                format=instance_log_format,
                level="INFO",
                rotation="10 MB",  # æ—¥å¿—æ–‡ä»¶å¤§å°è¾¾åˆ°10MBæ—¶è½®è½¬
                retention="7 days",  # ä¿ç•™7å¤©çš„æ—¥å¿—æ–‡ä»¶
                compression="zip",  # å‹ç¼©æ—§çš„æ—¥å¿—æ–‡ä»¶
                encoding="utf-8",
                filter=lambda record: record["extra"].get("instance_id") == self.instance_id
            )
            self.logger_ids.append(instance_file_handler_id)
            
            # æ·»åŠ æ–‡ä»¶è¾“å‡º - æ™®é€šæ—¥å¿—ï¼ˆæ²¡æœ‰å®ä¾‹IDçš„ï¼‰
            general_file_handler_id = logger.add(
                str(log_file),
                format=general_log_format,
                level="INFO",
                rotation="10 MB",  # æ—¥å¿—æ–‡ä»¶å¤§å°è¾¾åˆ°10MBæ—¶è½®è½¬
                retention="7 days",  # ä¿ç•™7å¤©çš„æ—¥å¿—æ–‡ä»¶
                compression="zip",  # å‹ç¼©æ—§çš„æ—¥å¿—æ–‡ä»¶
                encoding="utf-8",
                filter=lambda record: record["extra"].get("instance_id") is None
            )
            self.logger_ids.append(general_file_handler_id)
            
            # ç»‘å®šå®ä¾‹IDåˆ°logger
            logger.bind(instance_id=self.instance_id).info(f"å®ä¾‹ {self.instance_id} æ—¥å¿—é…ç½®å®Œæˆ - æ§åˆ¶å°è¾“å‡º + æ–‡ä»¶è¾“å‡º: {log_file}")
        else:
            # åªæœ‰æ§åˆ¶å°è¾“å‡º
            logger.bind(instance_id=self.instance_id).info(f"å®ä¾‹ {self.instance_id} æ—¥å¿—é…ç½®å®Œæˆ - ä»…æ§åˆ¶å°è¾“å‡º")

    def _cleanup_logging(self):
        """æ¸…ç†æ—¥å¿—å¤„ç†å™¨"""
        for handler_id in self.logger_ids:
            try:
                logger.remove(handler_id)
            except Exception as e:
                # å¿½ç•¥æ¸…ç†é”™è¯¯ï¼Œé¿å…å½±å“ç¨‹åºæ­£å¸¸é€€å‡º
                pass
        self.logger_ids.clear()

    def __del__(self):
        """ææ„å‡½æ•° - æ¸…ç†æ—¥å¿—å¤„ç†å™¨"""
        self._cleanup_logging()

    async def _initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        components_config = self.config.get('components', [])
        
        for i, comp_config in enumerate(components_config):
            comp_name = comp_config.get('name')
            comp_type = comp_config.get('type')
            
            try:
                # ä¸ºç¬¬ä¸€ä¸ªç»„ä»¶ï¼ˆæ•°æ®æºï¼‰æä¾›æ•°æ®å¤„ç†å™¨
                if i == 0 and comp_type == 'event_monitor':
                    component = ComponentFactory.create_component(
                        comp_type, comp_name, comp_config,
                        data_processor=self._process_pipeline_data
                    )
                else:
                    component = ComponentFactory.create_component(comp_type, comp_name, comp_config)
                
                # åˆå§‹åŒ–ç»„ä»¶
                if await component.initialize():
                    self.components.append(component)
                    logger.bind(instance_id=self.instance_id).info(f"ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ: {comp_name} ({comp_type})")
                else:
                    logger.error(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {comp_name} ({comp_type})")
                    
            except Exception as e:
                logger.error(f"åˆ›å»ºç»„ä»¶å¤±è´¥: {comp_name} ({comp_type}), é”™è¯¯: {e}")
                raise

    async def _process_pipeline_data(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†ç®¡é“æ•°æ® - è¿™æ˜¯äº‹ä»¶ç›‘æ§å™¨çš„å›è°ƒå‡½æ•°
        
        Args:
            event_data: æ¥è‡ªäº‹ä»¶ç›‘æ§å™¨çš„åŸå§‹äº‹ä»¶æ•°æ®
            
        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        try:
            # åˆ›å»ºç®¡é“ä¸Šä¸‹æ–‡
            context = PipelineContext(
                data=event_data.copy(),
                metadata={
                    'pipeline_name': self.config.get('pipeline_name'),
                    'start_time': datetime.now().isoformat(),
                    'instance_id': self.instance_id
                },
                pipeline_id=self.instance_id
            )
            
            logger.bind(instance_id=self.instance_id).info(f"å¼€å§‹å¤„ç†ç®¡é“æ•°æ®: {len(event_data)} ä¸ªå­—æ®µ")
            
            # æ‰“å°åˆå§‹æ•°æ®
            import json
            initial_json = json.dumps(context.data, ensure_ascii=False, indent=2, default=str)
            logger.bind(instance_id=self.instance_id).info(f"åˆå§‹è¾“å…¥æ•°æ®:")
            logger.bind(instance_id=self.instance_id).info(f"\n{initial_json}")
            
            # è·³è¿‡ç¬¬ä¸€ä¸ªç»„ä»¶ï¼ˆæ•°æ®æºï¼‰ï¼Œä»ç¬¬äºŒä¸ªç»„ä»¶å¼€å§‹æ‰§è¡Œ
            for component in self.components[1:]:
                try:
                    context = await component.execute(context)
                    logger.bind(instance_id=self.instance_id).info(f"ç»„ä»¶ {component.name} æ‰§è¡Œå®Œæˆ")
                    
                    # æ‰“å°æ¯ä¸ªæ­¥éª¤åçš„JSONæ•°æ®
                    step_json = json.dumps(context.data, ensure_ascii=False, indent=2, default=str)
                    logger.bind(instance_id=self.instance_id).info(f"ç»„ä»¶ {component.name} æ‰§è¡Œåçš„æ•°æ®:")
                    logger.bind(instance_id=self.instance_id).info(f"\n{step_json}")
                    
                except Exception as e:
                    logger.error(f"ç»„ä»¶ {component.name} æ‰§è¡Œå¤±è´¥: {e}")
                    # ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªç»„ä»¶ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
            
            logger.bind(instance_id=self.instance_id).info(f"ç®¡é“æ•°æ®å¤„ç†å®Œæˆ: æ‰§è¡Œäº† {context.step_count} ä¸ªæ­¥éª¤")
            return context.data
            
        except Exception as e:
            logger.error(f"ç®¡é“æ•°æ®å¤„ç†å¤±è´¥: {e}")
            return event_data

    async def execute_pipeline(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´ç®¡é“"""
        try:
            logger.bind(instance_id=self.instance_id).info(f"å¼€å§‹æ‰§è¡Œä¼˜åŒ–ç‰ˆç®¡é“: {self.config.get('pipeline_name', 'unknown')}")
            
            # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
            await self._initialize_components()
            
            if not self.components:
                raise ValueError("æ²¡æœ‰æˆåŠŸåˆå§‹åŒ–çš„ç»„ä»¶")
            
            # æ‰§è¡Œç¬¬ä¸€ä¸ªç»„ä»¶ï¼ˆæ•°æ®æºç»„ä»¶ï¼‰
            first_component = self.components[0]
            
            # åˆ›å»ºåˆå§‹ä¸Šä¸‹æ–‡
            initial_context = PipelineContext(
                data={},
                metadata={
                    'pipeline_name': self.config.get('pipeline_name'),
                    'start_time': datetime.now().isoformat(),
                    'instance_id': self.instance_id
                },
                pipeline_id=self.instance_id,
                task_id=self.task_id
            )
            
            # å¯åŠ¨æ•°æ®æºç»„ä»¶ï¼ˆé€šå¸¸æ˜¯é•¿æœŸè¿è¡Œçš„ï¼‰
            await first_component.execute(initial_context)
            
        except Exception as e:
            logger.error(f"ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
            raise
        finally:
            # æ¸…ç†èµ„æº
            await self._cleanup_components()

    async def _cleanup_components(self):
        """æ¸…ç†æ‰€æœ‰ç»„ä»¶èµ„æº"""
        for component in self.components:
            try:
                await component.cleanup()
                logger.bind(instance_id=self.instance_id).info(f"ç»„ä»¶æ¸…ç†å®Œæˆ: {component.name}")
            except Exception as e:
                logger.error(f"ç»„ä»¶æ¸…ç†å¤±è´¥: {component.name}, é”™è¯¯: {e}")
        
        # æ¸…ç†æ—¥å¿—å¤„ç†å™¨
        for handler_id in self.logger_ids:
            try:
                logger.remove(handler_id)
            except Exception:
                pass
        self.logger_ids.clear()

    def __del__(self):
        """ææ„å‡½æ•°"""
        # æ¸…ç†æ—¥å¿—å¤„ç†å™¨
        for handler_id in self.logger_ids:
            try:
                logger.remove(handler_id)
            except Exception:
                pass




async def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„ç®¡é“é…ç½®"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´çš„ç®¡é“é…ç½®")
    print("-" * 50)
    
    try:
        # åˆ›å»ºç®¡é“ï¼Œä½¿ç”¨å®Œæ•´é…ç½®æ–‡ä»¶
        config_path = "configs/optimized_pipeline_example.json"
        pipeline = OptimizedBlockchainDataPipeline(config_path)
        await pipeline.execute_pipeline()
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´ç®¡é“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    asyncio.run(test_full_pipeline())
