#!/usr/bin/env python3
"""
æµ‹è¯•å¸¦æ—¥å¿—è·¯å¾„çš„ start æ¥å£åŠŸèƒ½
"""

import asyncio
import json
import sys
import tempfile
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from app.services.database_service import database_service
from app.services.pipeline_config_service import PipelineConfigService


async def test_start_api_with_log_path():
    """æµ‹è¯•å¸¦æ—¥å¿—è·¯å¾„çš„ start æ¥å£"""
    
    print("ğŸ§ª æµ‹è¯•å¸¦æ—¥å¿—è·¯å¾„çš„ start æ¥å£åŠŸèƒ½")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“æœåŠ¡
        await database_service.init_db()
        
        # åˆ›å»ºä¸´æ—¶æ—¥å¿—æ–‡ä»¶è·¯å¾„
        temp_dir = Path(tempfile.mkdtemp())
        custom_log_path = temp_dir / "custom_pipeline.log"
        
        print(f"ğŸ“ ä¸´æ—¶æ—¥å¿—ç›®å½•: {temp_dir}")
        print(f"ğŸ“„ è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„: {custom_log_path}")
        
        async with database_service.get_session() as session:
            service = PipelineConfigService(session)
            
            # åˆ›å»ºæµ‹è¯•ç®¡é“é…ç½®
            test_config = {
                "pipeline_name": "æ—¥å¿—è·¯å¾„æµ‹è¯•ç®¡é“",
                "description": "æµ‹è¯•è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„åŠŸèƒ½",
                "classification_id": 1,
                "components": [
                    {
                        "name": "test_event_monitor",
                        "type": "event_monitor",
                        "chain_name": "ethereum",
                        "contract_address": "0xA0b86a33E6441c8C06DD2b7c94b7E0e8d61c6e8e",
                        "abi_path": "abis/erc20.json",
                        "mode": "realtime",
                        "events_to_monitor": ["Transfer"],
                        "output_format": "detailed",
                        "poll_interval": 1.0
                    },
                    {
                        "name": "test_dict_mapper",
                        "type": "dict_mapper",
                        "dict_mappers": [
                            {
                                "event_name": "Transfer",
                                "mapping_rules": [
                                    {
                                        "source_key": "from",
                                        "target_key": "sender_address",
                                        "transformer": "to_lowercase",
                                        "condition": None,
                                        "default_value": None
                                    },
                                    {
                                        "source_key": "to",
                                        "target_key": "receiver_address",
                                        "transformer": "to_lowercase",
                                        "condition": None,
                                        "default_value": None
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "name": "test_kafka_producer",
                        "type": "kafka_producer",
                        "bootstrap_servers": "localhost:9092",
                        "topic": "test_topic",
                        "acks": 1,
                        "sync_send": False
                    }
                ]
            }
            
            pipeline_id = 4001  # ä½¿ç”¨æ–°çš„æµ‹è¯•ç®¡é“ID
            pipeline_info_str = json.dumps(test_config, ensure_ascii=False, indent=2)
            
            print("\nğŸ”„ ä¿å­˜æµ‹è¯•ç®¡é“é…ç½®...")
            save_result = await service.parse_and_save_pipeline(pipeline_id, pipeline_info_str)
            print(f"âœ… ç®¡é“é…ç½®ä¿å­˜æˆåŠŸ: {save_result}")
            
            # æµ‹è¯•1: ä½¿ç”¨è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„å¯åŠ¨ç®¡é“
            print(f"\nğŸ”„ æµ‹è¯•1: ä½¿ç”¨è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„å¯åŠ¨ç®¡é“...")
            start_result = await service.start_pipeline_task(pipeline_id, str(custom_log_path))
            print(f"âœ… ç®¡é“å¯åŠ¨æˆåŠŸ: {start_result}")
            
            task_id = start_result.get('task_id')
            print(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç®¡é“åˆå§‹åŒ–
            print(f"â³ ç­‰å¾…ç®¡é“åˆå§‹åŒ–...")
            await asyncio.sleep(3)
            
            # æ£€æŸ¥è‡ªå®šä¹‰æ—¥å¿—æ–‡ä»¶æ˜¯å¦åˆ›å»º
            if custom_log_path.exists():
                file_size = custom_log_path.stat().st_size
                print(f"âœ… è‡ªå®šä¹‰æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {custom_log_path}")
                print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                
                # è¯»å–å¹¶æ˜¾ç¤ºæ—¥å¿—å†…å®¹çš„å‰å‡ è¡Œ
                print(f"\nğŸ“„ è‡ªå®šä¹‰æ—¥å¿—æ–‡ä»¶å†…å®¹é¢„è§ˆ:")
                print("-" * 40)
                with open(custom_log_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    for i, line in enumerate(lines[:8]):  # æ˜¾ç¤ºå‰8è¡Œ
                        print(f"{i+1:2d}: {line.rstrip()}")
                    
                    if len(lines) > 8:
                        print(f"... (è¿˜æœ‰ {len(lines) - 8} è¡Œ)")
                print("-" * 40)
            else:
                print("âŒ è‡ªå®šä¹‰æ—¥å¿—æ–‡ä»¶æœªåˆ›å»º")
                return False
            
            # æµ‹è¯•2: ä¸æŒ‡å®šæ—¥å¿—è·¯å¾„å¯åŠ¨ç®¡é“ï¼ˆä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
            print(f"\nğŸ”„ æµ‹è¯•2: ä½¿ç”¨é»˜è®¤æ—¥å¿—è·¯å¾„å¯åŠ¨ç®¡é“...")
            pipeline_id_2 = 4002
            
            # ä¿å­˜ç¬¬äºŒä¸ªç®¡é“é…ç½®
            test_config_2 = test_config.copy()
            test_config_2["pipeline_name"] = "é»˜è®¤æ—¥å¿—è·¯å¾„æµ‹è¯•ç®¡é“"
            pipeline_info_str_2 = json.dumps(test_config_2, ensure_ascii=False, indent=2)
            
            save_result_2 = await service.parse_and_save_pipeline(pipeline_id_2, pipeline_info_str_2)
            print(f"âœ… ç¬¬äºŒä¸ªç®¡é“é…ç½®ä¿å­˜æˆåŠŸ: {save_result_2}")
            
            # å¯åŠ¨ç®¡é“ï¼ˆä¸æŒ‡å®šæ—¥å¿—è·¯å¾„ï¼‰
            start_result_2 = await service.start_pipeline_task(pipeline_id_2)
            print(f"âœ… ç¬¬äºŒä¸ªç®¡é“å¯åŠ¨æˆåŠŸ: {start_result_2}")
            
            task_id_2 = start_result_2.get('task_id')
            print(f"ğŸ“‹ ç¬¬äºŒä¸ªä»»åŠ¡ID: {task_id_2}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´
            await asyncio.sleep(2)
            
            # æ£€æŸ¥é»˜è®¤æ—¥å¿—è·¯å¾„
            project_root = Path(__file__).parent
            default_log_pattern = f"logs/pipeline_{pipeline_id_2}_*.log"
            default_log_files = list(project_root.glob(default_log_pattern))
            
            if default_log_files:
                default_log_file = default_log_files[0]
                print(f"âœ… é»˜è®¤æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {default_log_file}")
                print(f"ğŸ“ æ–‡ä»¶å¤§å°: {default_log_file.stat().st_size} å­—èŠ‚")
                
                # æ¸…ç†é»˜è®¤æ—¥å¿—æ–‡ä»¶
                default_log_file.unlink()
                print(f"ğŸ§¹ æ¸…ç†é»˜è®¤æ—¥å¿—æ–‡ä»¶: {default_log_file}")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°é»˜è®¤æ—¥å¿—æ–‡ä»¶ï¼Œæ¨¡å¼: {default_log_pattern}")
            
            # æ¸…ç†è‡ªå®šä¹‰æ—¥å¿—æ–‡ä»¶
            if custom_log_path.exists():
                custom_log_path.unlink()
            temp_dir.rmdir()
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_relative_log_path():
    """æµ‹è¯•ç›¸å¯¹è·¯å¾„æ—¥å¿—åŠŸèƒ½"""
    
    print(f"\nğŸ§ª æµ‹è¯•ç›¸å¯¹è·¯å¾„æ—¥å¿—åŠŸèƒ½")
    print("-" * 35)
    
    try:
        async with database_service.get_session() as session:
            service = PipelineConfigService(session)
            
            # åˆ›å»ºç®€å•çš„æµ‹è¯•é…ç½®
            simple_config = {
                "pipeline_name": "ç›¸å¯¹è·¯å¾„æ—¥å¿—æµ‹è¯•",
                "description": "æµ‹è¯•ç›¸å¯¹è·¯å¾„æ—¥å¿—è¾“å‡º",
                "classification_id": 1,
                "components": [
                    {
                        "name": "simple_event_monitor",
                        "type": "event_monitor",
                        "chain_name": "ethereum",
                        "contract_address": "0xA0b86a33E6441c8C06DD2b7c94b7E0e8d61c6e8e",
                        "abi_path": "abis/erc20.json",
                        "mode": "realtime",
                        "events_to_monitor": ["Transfer"],
                        "output_format": "detailed"
                    }
                ]
            }
            
            pipeline_id = 4003
            pipeline_info_str = json.dumps(simple_config, ensure_ascii=False, indent=2)
            
            # ä¿å­˜é…ç½®
            save_result = await service.parse_and_save_pipeline(pipeline_id, pipeline_info_str)
            print(f"âœ… ç®¡é“é…ç½®ä¿å­˜æˆåŠŸ: {save_result}")
            
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            relative_log_path = "logs/test_relative_api.log"
            
            print(f"ğŸ“ ç›¸å¯¹æ—¥å¿—è·¯å¾„: {relative_log_path}")
            
            # å¯åŠ¨ç®¡é“
            start_result = await service.start_pipeline_task(pipeline_id, relative_log_path)
            print(f"âœ… ç›¸å¯¹è·¯å¾„ç®¡é“å¯åŠ¨æˆåŠŸ: {start_result}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´
            await asyncio.sleep(2)
            
            # æ£€æŸ¥å®é™…çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
            project_root = Path(__file__).parent
            expected_log_file = project_root / relative_log_path
            
            print(f"ğŸ“„ é¢„æœŸæ—¥å¿—æ–‡ä»¶è·¯å¾„: {expected_log_file}")
            
            if expected_log_file.exists():
                print(f"âœ… ç›¸å¯¹è·¯å¾„æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º")
                print(f"ğŸ“ æ–‡ä»¶å¤§å°: {expected_log_file.stat().st_size} å­—èŠ‚")
                
                # æ¸…ç†
                expected_log_file.unlink()
                print(f"ğŸ§¹ æ¸…ç†æ—¥å¿—æ–‡ä»¶: {expected_log_file}")
            else:
                print(f"âš ï¸  ç›¸å¯¹è·¯å¾„æ—¥å¿—æ–‡ä»¶æœªåˆ›å»º")
            
            return True
            
    except Exception as e:
        print(f"âŒ ç›¸å¯¹è·¯å¾„æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµ‹è¯•å¸¦æ—¥å¿—è·¯å¾„çš„ start æ¥å£åŠŸèƒ½")
    print("=" * 60)
    
    # æµ‹è¯•è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„å’Œé»˜è®¤æ—¥å¿—è·¯å¾„
    success1 = await test_start_api_with_log_path()
    
    # æµ‹è¯•ç›¸å¯¹è·¯å¾„æ—¥å¿—åŠŸèƒ½
    success2 = await test_relative_log_path()
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print("=" * 30)
    
    if success1:
        print("âœ… è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„æµ‹è¯•å¤±è´¥")
    
    if success2:
        print("âœ… ç›¸å¯¹è·¯å¾„æ—¥å¿—æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ ç›¸å¯¹è·¯å¾„æ—¥å¿—æµ‹è¯•å¤±è´¥")
    
    if success1 and success2:
        print(f"\nğŸ‰ æ‰€æœ‰æ—¥å¿—è·¯å¾„åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ“‹ éªŒè¯åŠŸèƒ½:")
        print("   âœ… start æ¥å£æ”¯æŒè‡ªå®šä¹‰æ—¥å¿—è·¯å¾„å‚æ•°")
        print("   âœ… è‡ªå®šä¹‰æ—¥å¿—è·¯å¾„æ­£ç¡®ä¼ é€’ç»™ç®¡é“æ‰§è¡Œå™¨")
        print("   âœ… ä¸æŒ‡å®šæ—¥å¿—è·¯å¾„æ—¶ä½¿ç”¨é»˜è®¤è·¯å¾„")
        print("   âœ… æ”¯æŒç›¸å¯¹è·¯å¾„æ—¥å¿—é…ç½®")
        print("   âœ… æ—¥å¿—æ–‡ä»¶æ­£ç¡®åˆ›å»ºå’Œå†™å…¥")
        print("   âœ… ä»»åŠ¡è®°å½•ä¸­ä¿å­˜æ­£ç¡®çš„æ—¥å¿—è·¯å¾„")
    else:
        print(f"\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")


if __name__ == "__main__":
    asyncio.run(main())
