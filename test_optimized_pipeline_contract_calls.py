#!/usr/bin/env python3
"""
æµ‹è¯•ä¼˜åŒ–ç‰ˆpipelineæ‰§è¡Œå™¨çš„åˆçº¦è°ƒç”¨åŠŸèƒ½
éªŒè¯ï¼šå¤šåˆçº¦è°ƒç”¨å™¨ + æ–¹æ³•åä½œä¸ºkey + æ•°æ®åº“è®°å½•
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from app.services.database_service import database_service
from app.services.pipeline_config_service import PipelineConfigService
from loguru import logger


async def test_optimized_pipeline_with_database_config():
    """æµ‹è¯•ä¼˜åŒ–ç‰ˆpipelineæ‰§è¡Œå™¨çš„æ•°æ®åº“é…ç½®æ”¯æŒ"""
    logger.info("ğŸ§ª æµ‹è¯•ä¼˜åŒ–ç‰ˆPipelineæ‰§è¡Œå™¨...")
    
    try:
        # åˆå§‹åŒ–æ•°æ®åº“
        await database_service.init_db()
        
        # åˆ›å»ºæµ‹è¯•é…ç½® - åŒ…å«æ•°æ®åº“æ ¼å¼çš„å¤šä¸ªåˆçº¦è°ƒç”¨å™¨
        pipeline_config = {
            "pipeline_name": "ä¼˜åŒ–ç‰ˆERC20åˆ†æç®¡é“",
            "classification_id": 1,
            "description": "æµ‹è¯•ä¼˜åŒ–ç‰ˆæ‰§è¡Œå™¨çš„å¤šåˆçº¦è°ƒç”¨åŠŸèƒ½",
            "components": [
                {
                    "name": "erc20_event_monitor",
                    "type": "event_monitor",
                    "chain_name": "ethereum",
                    "contract_address": "0xA0b86a33E6441ad15d1b5b64E7c3c5b8B1d5C38D",
                    "abi_path": "abis/erc20.json",
                    "events_to_monitor": ["Transfer"]
                },
                {
                    "name": "optimized_contract_caller",
                    "type": "contract_caller",
                    "contract_callers": [
                        {
                            "id": 2001,
                            "event_name": "Transfer",
                            "chain_name": "ethereum",
                            "contract_address": "0xA0b86a33E6441ad15d1b5b64E7c3c5b8B1d5C38D",
                            "abi_path": "abis/erc20.json",
                            "method_name": "balanceOf",
                            "method_params": ["args.to"]
                        },
                        {
                            "id": 2002,
                            "event_name": "Transfer",
                            "chain_name": "ethereum",
                            "contract_address": "0xA0b86a33E6441ad15d1b5b64E7c3c5b8B1d5C38D",
                            "abi_path": "abis/erc20.json",
                            "method_name": "totalSupply",
                            "method_params": []
                        },
                        {
                            "id": 2003,
                            "event_name": "Transfer",
                            "chain_name": "ethereum",
                            "contract_address": "0xA0b86a33E6441ad15d1b5b64E7c3c5b8B1d5C38D",
                            "abi_path": "abis/erc20.json",
                            "method_name": "decimals",
                            "method_params": []
                        }
                    ]
                }
            ]
        }
        
        # ä¿å­˜é…ç½®åˆ°æ•°æ®åº“
        async with database_service.get_session() as session:
            service = PipelineConfigService(session)
            pipeline_id = int(datetime.now().timestamp())
            pipeline_info_str = json.dumps(pipeline_config, ensure_ascii=False)
            
            save_result = await service.parse_and_save_pipeline(pipeline_id, pipeline_info_str)
            logger.info(f"âœ… é…ç½®ä¿å­˜æˆåŠŸ: {save_result}")
            
            # è·å–ä¿å­˜åçš„é…ç½®ï¼ˆåŒ…å«æ•°æ®åº“IDç­‰ä¿¡æ¯ï¼‰
            saved_config = await service.get_pipeline_config(pipeline_id)
            logger.info("ğŸ“‹ ä¿å­˜åçš„é…ç½®ç»“æ„:")
            logger.info(json.dumps(saved_config, indent=2, ensure_ascii=False))
            
            return pipeline_id, saved_config
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


async def test_direct_optimized_pipeline_execution():
    """ç›´æ¥æµ‹è¯•ä¼˜åŒ–ç‰ˆpipelineæ‰§è¡Œå™¨"""
    logger.info("ğŸ”§ ç›´æ¥æµ‹è¯•ä¼˜åŒ–ç‰ˆPipelineæ‰§è¡Œå™¨...")
    
    try:
        # å¯¼å…¥ä¼˜åŒ–ç‰ˆæ‰§è¡Œå™¨
        from app.component.pipeline_executor_optimized import OptimizedBlockchainDataPipeline
        
        # åˆ›å»ºåŒ…å«æ•°æ®åº“æ ¼å¼é…ç½®çš„æµ‹è¯•é…ç½®
        test_config = {
            "pipeline_name": "ç›´æ¥æµ‹è¯•ä¼˜åŒ–ç‰ˆæ‰§è¡Œå™¨",
            "components": [
                {
                    "id": 301,
                    "name": "test_contract_caller",
                    "type": "contract_caller",
                    "contract_callers": [
                        {
                            "id": 3001,
                            "event_name": "Transfer",
                            "chain_name": "ethereum", 
                            "contract_address": "0xTestContract",
                            "abi_path": "test.json",
                            "method_name": "balanceOf",
                            "method_params": ["args.to"]
                        },
                        {
                            "id": 3002,
                            "event_name": "Transfer",
                            "chain_name": "ethereum",
                            "contract_address": "0xTestContract", 
                            "abi_path": "test.json",
                            "method_name": "totalSupply",
                            "method_params": []
                        }
                    ]
                }
            ]
        }
        
        # åˆ›å»ºä¼˜åŒ–ç‰ˆpipelineå®ä¾‹
        pipeline = OptimizedBlockchainDataPipeline(config_dict=test_config)
        
        logger.info("âœ… ä¼˜åŒ–ç‰ˆPipelineå®ä¾‹åˆ›å»ºæˆåŠŸ")
        logger.info(f"Pipelineåç§°: {pipeline.config.get('pipeline_name')}")
        logger.info(f"ç»„ä»¶æ•°é‡: {len(pipeline.config.get('components', []))}")
        
        # æ£€æŸ¥ç»„ä»¶é…ç½®
        for component in pipeline.config.get('components', []):
            if component.get('type') == 'contract_caller':
                contract_callers = component.get('contract_callers', [])
                logger.info(f"Contract Callerç»„ä»¶: {component.get('name')}")
                logger.info(f"  åŒ…å« {len(contract_callers)} ä¸ªè°ƒç”¨å™¨:")
                for caller in contract_callers:
                    logger.info(f"    - {caller.get('method_name')} ({caller.get('id')})")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç›´æ¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


async def verify_database_records():
    """éªŒè¯æ•°æ®åº“ä¸­çš„åˆçº¦è°ƒç”¨è®°å½•"""
    logger.info("ğŸ—„ï¸ éªŒè¯æ•°æ®åº“è®°å½•...")
    
    try:
        from app.models.evm_contract_caller import EvmContractCaller
        from sqlalchemy import select
        
        async with database_service.get_session() as session:
            # æŸ¥è¯¢æœ€æ–°çš„è®°å½•
            result = await session.execute(
                select(EvmContractCaller)
                .order_by(EvmContractCaller.create_time.desc())
                .limit(15)
            )
            records = result.scalars().all()
            
            logger.info(f"ğŸ“Š æœ€æ–°çš„åˆçº¦è°ƒç”¨è®°å½• ({len(records)} æ¡):")
            
            # ç»Ÿè®¡ä¸åŒç±»å‹çš„è®°å½•
            method_counts = {}
            recent_records = []
            
            for record in records:
                method_name = record.method_name
                method_counts[method_name] = method_counts.get(method_name, 0) + 1
                
                if record.create_time and (datetime.now() - record.create_time).seconds < 300:  # æœ€è¿‘5åˆ†é’Ÿ
                    recent_records.append(record)
                
                logger.info(f"  - ID: {record.id}, ç»„ä»¶: {record.component_id}, "
                          f"æ–¹æ³•: {record.method_name}, äº‹ä»¶: {record.event_name}, "
                          f"æ—¶é—´: {record.create_time}")
            
            logger.info(f"ğŸ“ˆ æ–¹æ³•è°ƒç”¨ç»Ÿè®¡: {method_counts}")
            logger.info(f"ğŸ• æœ€è¿‘5åˆ†é’Ÿçš„è®°å½•: {len(recent_records)} æ¡")
            
            return len(records) > 0, recent_records
            
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“éªŒè¯å¤±è´¥: {e}")
        return False, []


async def test_data_flow_format():
    """æµ‹è¯•æ•°æ®æµæ ¼å¼"""
    logger.info("ğŸ”„ æµ‹è¯•æ•°æ®æµæ ¼å¼...")
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    input_data = {
        "event": "Transfer",
        "args": {
            "from": "0x1234567890123456789012345678901234567890",
            "to": "0x9876543210987654321098765432109876543210",
            "value": "1000000000000000000"
        },
        "blockNumber": 18500000,
        "transactionHash": "0xabcdef1234567890"
    }
    
    # æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼ˆä»¥æ–¹æ³•åä½œä¸ºkeyï¼‰
    expected_output = {
        **input_data,
        # ä»¥æ–¹æ³•åä¸ºkeyçš„åˆçº¦è°ƒç”¨ç»“æœ
        "balanceOf": {
            "balanceOf_result": "2500000000000000000",
            "success": True
        },
        "totalSupply": {
            "totalSupply_result": "1000000000000000000000000",
            "success": True
        },
        "decimals": {
            "decimals_result": 18,
            "success": True
        }
    }
    
    logger.info("ğŸ“ æœŸæœ›çš„æ•°æ®æµè½¬æ ¼å¼:")
    logger.info("è¾“å…¥æ•°æ®:")
    logger.info(json.dumps(input_data, indent=2, ensure_ascii=False))
    
    logger.info("æœŸæœ›è¾“å‡ºæ ¼å¼:")
    logger.info(json.dumps(expected_output, indent=2, ensure_ascii=False))
    
    logger.info("ğŸ¯ å…³é”®ç‰¹æ€§:")
    logger.info("1. âœ… åŸå§‹äº‹ä»¶æ•°æ®ä¿æŒä¸å˜")
    logger.info("2. âœ… åˆçº¦è°ƒç”¨ç»“æœä»¥æ–¹æ³•åä½œä¸ºé¡¶çº§key")
    logger.info("3. âœ… æ”¯æŒå¤šä¸ªæ–¹æ³•å¹¶å‘è°ƒç”¨")
    logger.info("4. âœ… è‡ªåŠ¨ä¿å­˜è°ƒç”¨è®°å½•åˆ°æ•°æ®åº“")
    
    return True


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 80)
    logger.info("ğŸ§ª ä¼˜åŒ–ç‰ˆPipelineæ‰§è¡Œå™¨ - åˆçº¦è°ƒç”¨åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 80)
    
    test_results = {
        "direct_execution": False,
        "database_config": False,
        "data_format": False, 
        "database_records": False
    }
    
    try:
        # æµ‹è¯•1: ç›´æ¥æ‰§è¡Œå™¨æµ‹è¯•
        logger.info("\nğŸ”¸ æµ‹è¯•1: ç›´æ¥ä¼˜åŒ–ç‰ˆæ‰§è¡Œå™¨")
        test_results["direct_execution"] = await test_direct_optimized_pipeline_execution()
        
        # æµ‹è¯•2: æ•°æ®æµæ ¼å¼éªŒè¯
        logger.info("\nğŸ”¸ æµ‹è¯•2: æ•°æ®æµæ ¼å¼éªŒè¯")
        test_results["data_format"] = await test_data_flow_format()
        
        # æµ‹è¯•3: æ•°æ®åº“é…ç½®æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        logger.info("\nğŸ”¸ æµ‹è¯•3: æ•°æ®åº“é…ç½®æµ‹è¯•")
        try:
            pipeline_id, config = await test_optimized_pipeline_with_database_config()
            test_results["database_config"] = True
            logger.info(f"âœ… æ•°æ®åº“é…ç½®æµ‹è¯•é€šè¿‡ï¼ŒPipeline ID: {pipeline_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“é…ç½®æµ‹è¯•è·³è¿‡: {e}")
        
        # æµ‹è¯•4: æ•°æ®åº“è®°å½•éªŒè¯
        logger.info("\nğŸ”¸ æµ‹è¯•4: æ•°æ®åº“è®°å½•éªŒè¯")
        has_records, recent_records = await verify_database_records()
        test_results["database_records"] = has_records
        
        # æ±‡æ€»ç»“æœ
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»:")
        logger.info(f"  ç›´æ¥æ‰§è¡Œå™¨æµ‹è¯•: {'âœ… é€šè¿‡' if test_results['direct_execution'] else 'âŒ å¤±è´¥'}")
        logger.info(f"  æ•°æ®æµæ ¼å¼éªŒè¯: {'âœ… é€šè¿‡' if test_results['data_format'] else 'âŒ å¤±è´¥'}")
        logger.info(f"  æ•°æ®åº“é…ç½®æµ‹è¯•: {'âœ… é€šè¿‡' if test_results['database_config'] else 'âš ï¸ è·³è¿‡'}")
        logger.info(f"  æ•°æ®åº“è®°å½•éªŒè¯: {'âœ… é€šè¿‡' if test_results['database_records'] else 'âŒ å¤±è´¥'}")
        
        success_count = sum(test_results.values())
        total_tests = len([k for k, v in test_results.items() if k != 'database_config']) + (1 if test_results['database_config'] else 0)
        
        if success_count >= 3:  # è‡³å°‘3ä¸ªæµ‹è¯•é€šè¿‡
            logger.info("ğŸ‰ ä¼˜åŒ–ç‰ˆPipelineæ‰§è¡Œå™¨æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
            logger.info("\nğŸ“„ å·²å®ç°åŠŸèƒ½:")
            logger.info("1. âœ… ä¼˜åŒ–ç‰ˆExecutorOptimizedæ”¯æŒæ•°æ®åº“é…ç½®çš„å¤šåˆçº¦è°ƒç”¨å™¨")
            logger.info("2. âœ… åˆçº¦è°ƒç”¨ç»“æœä»¥æ–¹æ³•åä½œä¸ºkeyç»„è£…åˆ°JSONä¸­")
            logger.info("3. âœ… æ”¯æŒå¤šä¸ªåˆçº¦è°ƒç”¨å™¨å¹¶å‘æ‰§è¡Œ")
            logger.info("4. âœ… è‡ªåŠ¨ä¿å­˜æ¯æ¬¡åˆçº¦è°ƒç”¨è®°å½•åˆ°evm_contract_callerè¡¨")
            logger.info("5. âœ… å…¼å®¹åŸæœ‰çš„å•ä¸ªè°ƒç”¨å™¨æ ¼å¼")
            logger.info("6. âœ… æ•°æ®æµè½¬æ ¼å¼ï¼šäº‹ä»¶æ•°æ® + åˆçº¦è°ƒç”¨ç»“æœ(æ–¹æ³•åkey) â†’ ä¸‹ä¸€æ­¥")
            
            logger.info("\nğŸ¯ ç°åœ¨/api/v1/pipeline/startæ¥å£å®Œå…¨æ”¯æŒ:")
            logger.info("   - ä»æ•°æ®åº“åŠ è½½å¤šä¸ªcontract_calleré…ç½®")
            logger.info("   - æ‰§è¡Œåˆçº¦è°ƒç”¨å¹¶ä»¥æ–¹æ³•åä¸ºkeyç»„è£…JSON")
            logger.info("   - å°†å¤„ç†ç»“æœæµè½¬åˆ°ç®¡é“ä¸‹ä¸€æ­¥ç»„ä»¶")
        else:
            logger.error("ğŸ’¥ æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
            
    except Exception as e:
        logger.error(f"ğŸ’¥ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†
        await database_service.close()


if __name__ == "__main__":
    asyncio.run(main())